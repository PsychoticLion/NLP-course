{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-02, Probability Model A First Look: An Introduction of Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Review the course online programming code; \n",
    "2. Review the main questions; \n",
    "3. Using wikipedia corpus to build a language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this part, you should re-code the programming task in our online course.*\n",
    "\n",
    "> Please see the file 'lec2_inclass_ex.ipynb'\n",
    "> \n",
    "\n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How to Github and Why do we use Jupyter and Pycharm; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Github是能够记录不同版本的代码，以便开发者进行开发。它先是在远程创建一个新的代码库，然后将本地的代码进行初始化提交。 在本地代码有变动时，可以将这些变动保存，然后提交至远程代码库，形成一个新的版本。\n",
    "    \n",
    "2.jupyter是浏览器代码编辑器，在jupyter里，开发者可以将代码分为一个个块运行，非常适合进行单个文件模块的开发与调试。\n",
    "\n",
    "3.Pycharm是一款python的编辑器，适用于多个模块文件的大项目开发与调试，同时也支持断点调试等功能。与jupyter相比，它具有更强大的开发支持\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 概率模型是从概率的角度思考问题，现在也是解决问题的三大思路之一，并越发受到重视。主要的三大思路：1.函数想法（得到一个函数描述变化规律）；2.以搜索为主的按图索骥式思路；3.在2的基础上通过大量数据，使用概率来解决问题，这种方法即使不知道内部的发展逻辑，只要数据量足够庞大，也可以得到不错的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动对话，自动文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模式搜索的思路的关键在于得到具体的模式图谱，所以它自身就具有很大的缺陷，一旦图谱发生部分的损坏或者数据越界，这种方法就将无法是从；概率的模型则没有了这些烦恼，因为不用再考虑具体的模式图谱，只要存在就可以得到概率，需要考虑与解决的只是概率的适宜程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将句子拆分为各个元素，从概率的角度考虑生成整个句子的可能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语音识别，文字识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-gram语言模型只考虑元素独立出现的概率而不考虑它们之间的联系关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.优点：结构简单，计算方便。\n",
    "2.缺点：没有考虑各元素间的相互联系，会出现偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.  What't the 2-gram models; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与1-gram类似，但这个模型考虑两个单词搭配出现时的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. what's the web crawler, and can you implement a simple crawler? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫可以自动的爬取页面上的信息，可以视为是对网页信息的提取操作。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.  There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种各样的人为反爬阻碍，解决问题方法是不要一次大量抓取，而是间隔的分段提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. What't the Regular Expression and how to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则表达式是一种对文本信息的过滤操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: You need to download the corpus from wikipedis:\n",
    "> https://dumps.wikimedia.org/zhwiki/20190401/\n",
    "\n",
    "Step 2: You may need the help of wiki-extractor:\n",
    "\n",
    "> https://github.com/attardi/wikiextractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据结构化处理（预处理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv\n",
    "import re\n",
    "\n",
    "def token(string):\n",
    "    return ''.join(re.findall('[\\w|\\d]+',string))\n",
    "\n",
    "inputFile = open(\"./output_zhwiki.txt\",encoding='gb18030')                  #不同的对象带来不同的操作\n",
    "\n",
    "a=inputFile.read()\n",
    "\n",
    "text=token(str(a))\n",
    "TEXT=text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Using the technologies and methods to finish the language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.859 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "ALL_TOKEN=cut(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valida_tokens = [t for t in ALL_TOKEN if t.strip() and t != 'n']\n",
    "words_count = Counter(valida_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20866800\n"
     ]
    }
   ],
   "source": [
    "frequences_all = [f for w, f in words_count.most_common()]\n",
    "frequences_sum = sum(frequences_all)\n",
    "print(frequences_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(word): \n",
    "    esp = 1 / frequences_sum\n",
    "    if word in words_count: \n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])          #括号什么的注意写全"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(S) = Pr(w_1\\cdot w_2 \\cdots w_n) \\sim Pr(w_1) \\cdot Pr(w_2 | w_1) \\cdots Pr(w_n | w_{n-1})$$\n",
    "\n",
    "$$ Pr(w_2 | w_1) =  \\frac { Pr(w_1 \\cdot w_2) }{Pr(w_1)} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens = [str(t) for t in valida_tokens] \n",
    "all_2_grams_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "  \n",
    "    if w1 + w2 in _2_gram_counter: return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum\n",
    "    \n",
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "def langauge_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words_2_gram = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words_2_gram):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words_2_gram[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability                             #思路要清晰，哪些部分单元素为基本单元（被预测输入），哪些部分组合元素为基本单元（数据库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.828884782883177e-19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langauge_model_of_2_gram('小明今天抽奖抽到一台苹果手机')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(S) = Pr(w_1\\cdot w_2 \\cdots w_n) \\sim Pr(w_1 | *,*) \\cdot Pr(w_2 | *,w_1) \\cdot Pr(w_3 | w_1,w_2) \\cdots Pr(w_n | w_{n-2},w_{n-1})$$\n",
    "\n",
    "$$ Pr(w_3 | w_1,w_2)=  \\frac { Pr(w_3 \\cdot (w_1,w_2)) }{Pr(w_1,w_2)} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3_grams_words = [''.join(valid_tokens[i:i+3]) for i in range(len(valid_tokens[:-3]))]\n",
    "#三元素组成最小单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3_gram_sum = len(all_3_grams_words)\n",
    "_3_gram_counter = Counter(all_3_grams_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_combination_prob(w1, w2,w3):                         #!!!!\n",
    "   \n",
    "    if w1+w2+w3 in _3_gram_counter:        \n",
    "        return _3_gram_counter[w1+w2+w3]/_3_gram_sum\n",
    "    \n",
    "    elif w1+w2 in _2_gram_counter:         \n",
    "              \n",
    "        return  _2_gram_counter[w1+w2] / _2_gram_sum \n",
    "    \n",
    "    else:\n",
    "        return 1 / _3_gram_sum\n",
    "    \n",
    "    \n",
    "def get_prob_3_gram(w1, w2,w3):\n",
    "    return get_3_combination_prob(w1,w2,w3) / get_prob_2_gram(w1,w2)\n",
    "\n",
    "def langauge_model_of_3_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words_3_gram = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words_3_gram):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        if i==1:\n",
    "            prob = get_prob_2_gram(words_3_gram[i-1],word)\n",
    "        else:\n",
    "            moreprevious = words_3_gram[i-2]\n",
    "            previous = words_3_gram[i-1]\n",
    "            prob = get_prob_3_gram(moreprevious,previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0747950210022668e-69\n",
      "3.572351677215037e-77\n"
     ]
    }
   ],
   "source": [
    "print(langauge_model_of_3_gram('小明今天抽奖抽到一台苹果手机'))\n",
    "print(langauge_model_of_3_gram('小明今天抽奖抽到一台播音飞机'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: If we need to solve following problems, how can language model help us? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.优点：可以有效解决绝大多数问题，代码较为简便\n",
    "2.缺点：依托于先前的数据库，一旦发生越界，会造成较大的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)  How to solve *OOV* problem?\n",
    "\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this `out-of-vocabulary`(OOV) problems. There are so many intelligent man to solve this probelm. \n",
    "\n",
    "-- \n",
    "\n",
    "The first question is: \n",
    "\n",
    "**Q1: How did you solve this problem in your programming task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the sencond question is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
